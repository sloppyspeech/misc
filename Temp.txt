To compare two Snowflake databases with 6000 tables and 10 PB of data efficiently, follow this structured approach using Python:

### **1. Schema Comparison**
**Objective**: Verify that all tables and their structures (columns, data types, constraints) match between databases.

```python
import snowflake.connector
from snowflake.connector import DictCursor

def get_table_schema(conn, database, schema, table):
    query = f"""
    SELECT 
        column_name, 
        data_type, 
        is_nullable, 
        ordinal_position
    FROM {database}.INFORMATION_SCHEMA.COLUMNS
    WHERE table_schema = '{schema}' AND table_name = '{table}'
    ORDER BY ordinal_position;
    """
    cursor = conn.cursor(DictCursor).execute(query)
    return cursor.fetchall()

def compare_schemas(source_conn, target_conn, database, schema, table):
    source_schema = get_table_schema(source_conn, database, schema, table)
    target_schema = get_table_schema(target_conn, database, schema, table)
    
    if len(source_schema) != len(target_schema):
        return False, "Column count mismatch"
    
    for src_col, tgt_col in zip(source_schema, target_schema):
        if (src_col['column_name'] != tgt_col['column_name'] or
            src_col['data_type'] != tgt_col['data_type'] or
            src_col['is_nullable'] != tgt_col['is_nullable']):
            return False, f"Column {src_col['column_name']} mismatch"
    
    return True, "Schema matches"
```

### **2. Row Count Comparison**
**Objective**: Ensure the number of rows in each table matches.

```python
def compare_row_counts(source_conn, target_conn, database, schema, table):
    query = f"SELECT COUNT(*) FROM {database}.{schema}.{table};"
    source_count = source_conn.cursor().execute(query).fetchone()[0]
    target_count = target_conn.cursor().execute(query).fetchone()[0]
    return source_count == target_count, (source_count, target_count)
```

### **3. Data Sampling & Statistical Comparison**
**Objective**: Compare a subset of data using date ranges and statistical metrics.

#### **3.1 Identify Date Column**
Automatically detect a date column for sampling (e.g., `created_at`).

```python
def get_date_column(conn, database, schema, table):
    query = f"""
    SELECT column_name
    FROM {database}.INFORMATION_SCHEMA.COLUMNS
    WHERE table_schema = '{schema}' 
      AND table_name = '{table}'
      AND data_type IN ('DATE', 'TIMESTAMP_NTZ', 'TIMESTAMP_LTZ')
    LIMIT 1;
    """
    cursor = conn.cursor().execute(query)
    result = cursor.fetchone()
    return result[0] if result else None
```

#### **3.2 Generate Stats Query**
Build a dynamic SQL query to compute column statistics.

```python
def generate_stats_query(table_ref, date_col, start_date, end_date):
    columns = get_table_columns(source_conn, database, schema, table)
    query_parts = []
    
    for col in columns:
        col_name = col['column_name']
        data_type = col['data_type']
        
        if data_type in ('NUMBER', 'INT', 'FLOAT', 'DECIMAL'):
            stats = f"""
            SELECT '{col_name}' AS column_name,
                   MIN({col_name}) AS min_val,
                   MAX({col_name}) AS max_val,
                   AVG({col_name}) AS avg_val
            FROM {table_ref}
            WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'
            """
        elif data_type in ('VARCHAR', 'STRING', 'TEXT'):
            stats = f"""
            SELECT '{col_name}' AS column_name,
                   MIN(LENGTH({col_name})) AS min_length,
                   MAX(LENGTH({col_name})) AS max_length,
                   COUNT(DISTINCT {col_name}) AS distinct_count
            FROM {table_ref}
            WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'
            """
        elif data_type in ('DATE', 'TIMESTAMP'):
            stats = f"""
            SELECT '{col_name}' AS column_name,
                   MIN({col_name}) AS min_date,
                   MAX({col_name}) AS max_date
            FROM {table_ref}
            WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'
            """
        query_parts.append(stats)
    
    return " UNION ALL ".join(query_parts)
```

#### **3.3 Compare Stats**
Execute the query on both databases and compare results.

```python
def compare_data_stats(source_conn, target_conn, query):
    source_stats = source_conn.cursor(DictCursor).execute(query).fetchall()
    target_stats = target_conn.cursor(DictCursor).execute(query).fetchall()
    
    discrepancies = []
    for src, tgt in zip(source_stats, target_stats):
        if src != tgt:
            discrepancies.append((src['column_name'], src, tgt))
    
    return discrepancies
```

### **4. Orchestration & Parallelization**
Use parallel processing to handle 6000 tables efficiently.

```python
from concurrent.futures import ThreadPoolExecutor

def compare_tables(source_conn, target_conn, database, schema, tables):
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for table in tables:
            future = executor.submit(
                compare_single_table,
                source_conn, target_conn, database, schema, table
            )
            futures.append(future)
        
        for future in futures:
            # Log results or handle exceptions
            future.result()

def compare_single_table(source_conn, target_conn, database, schema, table):
    # Step 1: Schema comparison
    schema_match, schema_msg = compare_schemas(...)
    if not schema_match:
        log_discrepancy(f"Schema mismatch in {table}: {schema_msg}")
        return
    
    # Step 2: Row count comparison
    count_match, (source_count, target_count) = compare_row_counts(...)
    if not count_match:
        log_discrepancy(f"Row count mismatch in {table}: {source_count} vs {target_count}")
        return
    
    # Step 3: Data stats comparison
    date_col = get_date_column(...)
    if not date_col:
        log_discrepancy(f"No date column found in {table}")
        return
    
    query = generate_stats_query(...)
    discrepancies = compare_data_stats(...)
    if discrepancies:
        log_discrepancy(f"Data stats mismatch in {table}: {discrepancies}")
```

### **5. Statistical Methods for All Data Types**
- **Numeric Columns**: Compare `min`, `max`, `avg`, and `stddev`.
- **String Columns**: Compare `min_length`, `max_length`, and `distinct_count`.
- **Date Columns**: Compare `min_date` and `max_date`.

### **6. Reporting & Logging**
Store results in a structured format (e.g., CSV, database table) for auditing.

```python
import csv

def log_discrepancy(message):
    with open("discrepancies.csv", "a") as f:
        writer = csv.writer(f)
        writer.writerow([message])
```

### **Key Optimizations**
1. **Dynamic Query Generation**: Minimize round-trips to Snowflake by computing all stats in a single query.
2. **Parallel Execution**: Use `ThreadPoolExecutor` to compare multiple tables concurrently.
3. **Date-Based Sampling**: Focus on recent data (e.g., `WHERE date_col > CURRENT_DATE - 30`).
4. **Efficient Type Handling**: Leverage Snowflake’s metadata to tailor stats per column type.

### **Execution Steps**
1. **Install Dependencies**:
   ```bash
   pip install snowflake-connector-python
   ```
2. **Configure Connections**:
   ```python
   source_conn = snowflake.connector.connect(
       user=USER, password=PASSWORD, account=ACCOUNT, 
       database=SOURCE_DB, schema=SCHEMA
   )
   target_conn = snowflake.connector.connect(...)
   ```
3. **Run Comparison**:
   ```python
   tables = get_all_tables(source_conn)
   compare_tables(source_conn, target_conn, "MY_DB", "PUBLIC", tables)
   ```

This approach balances performance and thoroughness by leveraging Snowflake’s compute power for statistical aggregations and Python for orchestration.
